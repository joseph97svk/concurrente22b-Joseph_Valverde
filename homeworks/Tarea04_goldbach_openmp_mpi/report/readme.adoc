= Report =

== Summary ==

All tests were done on the Arenal cluster. The unit used was unit 0, which at the time was confirmed to have had no other users at the moment. The device counts with a CPU which has 8 cores and 8 threads. 

The test used was input029.txt, which is a combination of all small and medium tests in single file for testing. The amount of numbers for testing in this file is 382.

== Optimization 1: serial ==

=== Design ===

Most of the time processing the goldbach sums was found to be spent on calculating prime numbers. Aside from this, for every sum, every number had to be calculated to check if it was prime, and this for every number whose sums was to found. As a result, 2 optimizations were chosen as needed: to better calculate or find the prime numbers, and to only do so once.

For the first point, of increasing the efficiency of the prime number search, instead of a prime number seach where each number was checked for primality, a sieve was chosen. Sieves are found to be the most time efficient way to find large amount of prime numbers and the chosen sieve, the sieve of Atkins, has an O time of O(n). The only caveat would be the large amount of space necessary to store said numbers, as these are stored in an array of bools. Since this array stores wether a number is prime or not, each access is a simple O(1). In the case where the memory could not be allocated to store this array, the system would default to the previous primality test method.

For the second point on only calculating said numbers next, the previously mentioned array would be created once at the begining of execution and all numbers would then read from this array. To make sure all numbers have their sums in the array, on input, the highest number would be stored in shared data structures. Once input is done, all prime numbers up to this number would be calculated. Then afterwards, all numbers would read from this array. 

=== Speed up ===

Comparing the previous implementation, using 1 thread, and this optimization, once again using 1 thread, the speed up was found to be of around 43 times, with the original time being 430 seconds and the new one about 9 seconds. This indicates that indeed the processing of the prime numbers and the redundancy of this calculations was a bottleneck or the main culprit for most of the execution time. 

=== Efficiency ===

According to the formula (speed up)/(core amount), the efficiency was found to be of 43.72. This might be found to extreme, however, the amount of cores being used is the same, but the time taken in 43 times less, so rather than considering the optimization to be of efficiency 43.72, it would be better to interpret this as it is 43.72 times MORE efficient than the original implementation. 

=== Lessons learned ===

The bottlenecks found by the analisis are confirmed to have been correct. The increase in terms of the speed up and efficiency indicate so. By removing the redundancy of operations, uncessary repeated calculations as discarded and the use of resources could be more optimally use. The result of these optimizations are able to lead to great reductions in runtime. 


== Optimization 2: Pthreads/Dynamic Mapping == 

=== Design ===

This optimizations were already present in the previous design. As the use of Pthreads was implemented through the use of dynamic mapping, these two optimizations are to be reported together in this section. 

As mentioned in the analisis, the choice of what to parallelize was a compromise and the benefits depend on the use case. The implementation depends of each number, whose sums are to be found, to be processed by a thread each. As such this implementation benefits from the calculation of greater amounts of numbers rather than a single large one. 

The implementation of the dynamic mapping is found at the stage of each of these previously mentioned threads finding which number to process. These numbers are all stored within an internal array. For each of these threads to find the next available number, an shared counter indicates the next number to be processed. As each thread searches for a new number to be proccessed, this counter is increased. As such, the first thread to ask for a number, will receive a number to be processed. Once this counter reaches the amount of numbers, it will not be increased, and each thread will know there is no more work to be done. 

=== Speed up ===

The use of this optimization was found, to speed up performance, related to the first realease, to arround 6 times. Given the test having been run on a machine with 8 cores, it is certainly less than the amount of cores given, however the reduction in time remains substantial.

=== Efficiency ===

As mentioned in the previous section, the amount of cores is greater than the reduction in time from the optimization. The result is an efficiency of 0.73, which is less than that of the previous optimization and even less than the original release. This may be due to the balancing of the workload leading to a less than optimal distribution of the resources. Regardless of the efficiency, the speed up could be considered substantial enough to be considered bneneficial. 

=== Lessons learned ===

The correct balancing of the workload is essential in the parallelization of workloads. In this case, it could be considered that the units distributed were too coarse to be distributed correctly. The current concurrent solution also means the first optimization renders this second one less efficient. 

Another caveat learned from this is that there are substantial amounts in the processing line that are still single threaded and as such, any change or rebalancing of the pipeline could lead to the optimization/solution being rendered redundant or even detrimental.

== OpenMP == 

=== Design ===

The use of openMp required little changes in the previously implemented version. It required the removal of all previosly set concurrency controls and threading controls. Given that a single for, to be used on all numbers, once all prime numbers have been calculated, was used. This for was parallelized with the OpenMp technology which would distribute the tasks of the for loop to each thread. 

=== Speed up ===

This implementation resulted in a speed up, when compared to the previous Pthread implementation, even the optimized version. The declarative nature of the OpenMp leads to opacity to as for why, but, given the previous constraint to use only dynamic mapping, it can be explained that less concurrency controls lead to better performance.

=== Efficiency ===

Given the increase in performance without the increase in resources, the OpenMp version is, among all tested implementations, the most efficient one.

=== Lessons learned ===

As explained in the speed up section, there is opacity as to the reasons of the results. However the choice of the correct mapping and the reduction of concurrency controls due this, can be seen as a reason and as lesson to be considered when choosing how to distrubute the processing of data. 

== MPI == 

=== Design ===

This implementation required larger changes of the design due to the use of several processes. From the begining of execution it became necessary to segregate the tasks of each process. As such the first step became to identificate each process and assign tasks.

For this, process 0 was chosen as an administrator and distrubutor. Process 0 was responsabilized with reading the numbers from the input and then once this data is in the structures find which process is available to process. The mapping for the distrubition of data to be processed is a hybrid between block and dynamic mappings. A block is designated according to the amount of processes that have collected their data. However these blocks are dynamically distrubuted according to which process has arrived first to collect this data. An array of ranks is kept to synchronize the order of data to be printed.

Each other process then processess first the prime numbers for the numbers they have and then the sums for each of the numebers assigned. These then send to process 0 the amount of sums, where once collected, are then printed as a total. With these then process 0 sends signals for each process, in order, to print their results.

=== Speed up ===

There is a speed up, but only when comparted with the serial version and the concurrent version with dynamic mapping. This, however, is not due to the nature of this implementation but rather, of the optimizations present for the serial part of the processing pipeline which were introduced in the Optimization 1 version. The reasoning for the lack of speed up is the distribution of data between processes which have to then coordinate without shared memory and the test case which is unable to correctly saturate the allocated resources. 

=== Efficiency ===

This version also has the least efficiency out of all tested versions, given the lack of a greater increase in performance to go with the increase in resources. Further reasoning is already explained in the speed up section and will be further expanded in the comparisons section of this report. 

=== Lessons learned ===

The use case is as important as the implementation. The current test cases are unable to properly use the resources for this implementation which is designed for large amounts of data, each of which needs to be sufficiently large too. 

== Comparison 01: Optimizations ==
[#ChartOptimizations.png]
image::ChartOptimizations.png[]

[#ChartOptimizations1.png]
image::ChartOptimizations1.png[]

The results of all tests together seem to be as expected. In terms of time taken, the original serial version is the slowest at more than 7 minutes, the concurrent version being faster and the optimization part being the fastest. The speed up is a reflection of this behaviour, with the greatest increase in the speed up being with the optimization version. There is however an intersection between time and speed up at the concurrent version with dynamic mapping.

When comparing the speed up and the efficiency, these are largely similar, with an intersection too at the current and dynamic mapping version. This comparison, however shows a greater benefit for the optimized version, the efficiency shoots up to, in line with the speed up. The intersection at the concurrent version with dynamic mapping may indicate that as an optimal solution is a balance between both is desired. However if an increase in performance and efficiency is desired, the speed up found between the concurrent version and the optimized version may indicate this as the optimal solution for said case. 

[#tableOptimizations.png]
image::TableOptimizations.png[]



== Comparison 02: Degrees of concurrency ==
[#ChartConcurrencyLevel.png]
image::ChartConcurrencyLevel.png[]

[#ChartConcurrencyLevel1.png]
image::ChartConcurrencyLevel1.png[]

The comparisons in degrees of efficiency are all done using the optimized version. The results of this comparison could be considered rather bizare. The amount of threads with the least execution time is when just one is used. The greatest amount of time taken is when using all the cores that the system has, 8. Increasing the amount of cores then leads to increase in the speed up from this point, however the time taken is still greater than when using one core and begins flatlining at 32 threads. 

When comparing the speed up and the effiency, no intersection could be found. This comparison also finds that increasing the amount of cores is detrimental to, not only performance, but efficiency.

Different machines have show different behaviour, however to keep consistency, as these are all done in the same machine as the previous tests, we'll continue using this. This discrepancy may indicate that the system architecture may be influencing the results. 

One plausible explanation for this bahaviour is found in what sections are parallelized and which ones are serial. For this optimization the prime number search was optimized but serialized. This took load off the section that was previously parallelized, which reduced its impact and might have even began hurting performance. Since the threads have less to do, their positive impact has been reduced and the necesary resources for their concurrent functioning, instead began taking time that was previously justified by their time reduction.



[#tableConcurrencyLevel.png]
image::TableConcurrencyLevel.png[]

== Comparisons 03 - 04 ==
[#ChartOptimizations3.png]
image::ChartOptimizations2.png[]

[#ChartOptimizations4.png]
image::ChartOptimizations4.png[]

[#tableOptimizations2.png]
image::TableOptimizations2.png[]

=== Comparison 03: Pthreads-OpenMP

The OpenMp implementation lead to a decrease of the time taken in seconds for the processing of the goldbach numbers when compared to the Pthreads implementation in Optimization 1. The previous implementation received no benefit from the use of concurrency due to the heavy use of serial optimizations. The implementation of OpenMp, however, is unknown due to its declarative nature, making finding the reasoning for these results obscure. The resulting speedup may be the result of less cocurrency controls and better distribution leading to less stalls and serializations of the work. This might be seen as in the previous pthread implementation, required the use of concurrency controls for the distribution of work that was already too brief to benefit from parallelism.

Regardless of the reasoning behing the speedup, given the increase in performance without any increase of the resources allocated, the OpenMp implementation is, within all optimizations performed, the most efficient one. It is also the one with the greatest speedup in relation to the serial version. Comparisons with the concurrent version with dynamic mapping are irrelevant given the fact that this version lacks the optimizations in the prime numbe calculation that Optimization 1 already counts on, making this comparison unfair. 

=== Comparison 04: OpenMP-MPI ===

The MPI implementation results comes as both a surprise as much as completely expected. Giving more resources did not lead to an increase in performance in relation to the OpenMp version which is also present as the multi-threaded implementation of this version. This version ended up slower thatn the OpenMP version and the Optimization 1 version, but still faster than the first concurrent version with dynamic mapping and the serial version. 

The unexpected nature of the results is based on the amount of resources allocated for the processing of the data. 3 machines with 8 cores each were responsible for the processing of the data and 1 machine with 4 cores (of which only one is used) was tasked to distribute and coordinate the other 3 processes. A total 24 cores were tasked with the processing of the prime numbers. 

The expected part results in known architectural and implementation details as well as knownledge of the current test case. The main process is tasked with the distribution of tasks which, over to other machines, adds latency. Each machine calculates all necessary prime numbers and are then read for each number that needs them. The numbers in the test cases are unable to saturate every machine and each of its cores, leading to underutilization and waste of resources. Something which is clearly seen in the abysmal efficiency results. 

Overall the results, in comparison with other past resuls, are not indicative of a bad implementation or wastefulness of process distribution, but rather of the importance of the choice of tasks. Given smaller tasks, single machines or single cores are better suited. Given a task with a large amount of large numbers, would, theoretically lead to speedups one would expect from these configurations. 